{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class QuadrotorEnv(gym.Env):\n",
    "#     metadata = {'render.modes': ['console']}\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(QuadrotorEnv, self).__init__()\n",
    "\n",
    "#         # Constants\n",
    "#         self.g = 9.81  # gravity\n",
    "#         self.m = 1.0   # mass of the UAV\n",
    "#         self.mu = 0.05 # damping factor\n",
    "#         self.dt = 0.02 # time step\n",
    "\n",
    "#         # Define action and observation space\n",
    "#         # Actions are thrust T, angle phi, angle theta\n",
    "#         self.action_space = spaces.Box(low=np.array([0, -np.pi, -np.pi]), \n",
    "#                                        high=np.array([20, np.pi, np.pi]), dtype=np.float32)\n",
    "\n",
    "#         # Observation space: x, y, z, vx, vy, vz\n",
    "#         self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)\n",
    "\n",
    "#         # State initialization\n",
    "#         self.state = None\n",
    "#         self.reset()\n",
    "\n",
    "#     def step(self, action):\n",
    "#         T, phi, theta = action\n",
    "#         x, y, z, vx, vy, vz = self.state\n",
    "\n",
    "#         # Calculate accelerations\n",
    "#         ax = (-0.7071 * np.cos(phi) * np.sin(theta) - 0.7071 * np.sin(phi)) * T / self.m\n",
    "#         ay = (-0.7071 * np.cos(phi) * np.sin(theta) - 0.7071 * np.sin(phi)) * T / self.m\n",
    "#         az = self.g - (np.cos(phi) * np.cos(theta)) * T / self.m\n",
    "\n",
    "#         # Update velocities\n",
    "#         vx += (ax - self.mu * vx) * self.dt\n",
    "#         vy += (ay - self.mu * vy) * self.dt\n",
    "#         vz += (az - self.mu * vz) * self.dt\n",
    "\n",
    "#         # Update positions\n",
    "#         x += vx * self.dt\n",
    "#         y += vy * self.dt\n",
    "#         z += vz * self.dt\n",
    "\n",
    "#         # Update state\n",
    "#         self.state = np.array([x, y, z, vx, vy, vz])\n",
    "\n",
    "#         # Calculate reward (placeholder)\n",
    "#         reward = -np.sqrt((x - 5 * np.cos(1.2 * self.current_step * self.dt))**2 + (y - 5 * np.sin(1.2 * self.current_step * self.dt))**2 + (z + 20)**2)\n",
    "\n",
    "#         # Check if UAV is within the reasonable bounds (this is a simple check)\n",
    "#         done = z < -25 or self.current_step > 1000\n",
    "\n",
    "#         self.current_step += 1\n",
    "\n",
    "#         # Optionally we could add more info\n",
    "#         info = {}\n",
    "\n",
    "#         return self.state, reward, done, info\n",
    "\n",
    "#     def reset(self):\n",
    "#         # Reset the state\n",
    "#         self.state = np.array([0.0, 0.0, 0.0, 1.0, -1.0, 0.0], dtype=np.float32)\n",
    "#         self.current_step = 0\n",
    "#         return self.state\n",
    "\n",
    "#     def render(self, mode='console'):\n",
    "#         if mode == 'console':\n",
    "#             print(f'State: {self.state}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Quadrotor Environment\n",
    "class QuadrotorEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['console']}\n",
    " \n",
    "    def __init__(self):\n",
    "        super(QuadrotorEnv, self).__init__()\n",
    "        self.g = 9.81  \n",
    "        self.m = 1.0   \n",
    "        self.mu = 0.05\n",
    "        self.dt = 0.02\n",
    "        self.action_space = gym.spaces.Box(low=np.array([0, -np.pi, -np.pi]), \n",
    "                                           high=np.array([20, np.pi, np.pi]), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)\n",
    "        self.reset()\n",
    " \n",
    "    def step(self, action):\n",
    "        T, phi, theta = action\n",
    "        x, y, z, vx, vy, vz = self.state\n",
    "        ax = (-0.7071 * np.cos(phi) * np.sin(theta) - 0.7071 * np.sin(phi)) * T / self.m\n",
    "        ay = (-0.7071 * np.cos(phi) * np.sin(theta) - 0.7071 * np.sin(phi)) * T / self.m\n",
    "        az = self.g - (np.cos(phi) * np.cos(theta)) * T / self.m\n",
    "        vx += (ax - self.mu * vx) * self.dt\n",
    "        vy += (ay - self.mu * vy) * self.dt\n",
    "        vz += (az - self.mu * vz) * self.dt\n",
    " \n",
    "        x += vx * self.dt\n",
    "        y += vy * self.dt\n",
    "        z += vz * self.dt\n",
    " \n",
    "        self.state = np.array([x, y, z, vx, vy, vz])\n",
    "        reward = -np.sqrt((x - 5 * np.cos(1.2 * self.current_step * self.dt))**2 + \n",
    "                          (y - 5 * np.sin(1.2 * self.current_step * self.dt))**2 + (z + 20)**2)\n",
    "        done = z < -50 or self.current_step > 1000\n",
    "        self.current_step += 1\n",
    "        return self.state, reward, done, {}\n",
    " \n",
    "    def reset(self):\n",
    "        self.state = np.array([0.0, 0.0, 0.0, 1.0, -1.0, 0.0], dtype=np.float32)\n",
    "        self.current_step = 0\n",
    "        return self.state\n",
    " \n",
    "    def render(self, mode='console'):\n",
    "        if mode == 'console':\n",
    "            print(f'State: {self.state}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# # Actor Network\n",
    "# class Actor(nn.Module):\n",
    "#     def __init__(self, state_dim, action_dim, action_bound):\n",
    "#         super(Actor, self).__init__()\n",
    "#         self.layer1 = nn.Linear(state_dim, 256)\n",
    "#         self.layer2 = nn.Linear(256, 256)\n",
    "#         self.mean = nn.Linear(256, action_dim)\n",
    "#         self.log_std = nn.Linear(256, action_dim)\n",
    "#         self.action_bound = action_bound\n",
    " \n",
    "#     def forward(self, state):\n",
    "#         x = torch.relu(self.layer1(state))\n",
    "#         x = torch.relu(self.layer2(x))\n",
    "#         mean = self.mean(x)\n",
    "#         log_std = self.log_std(x)\n",
    "#         log_std = torch.clamp(log_std, -20, 2)\n",
    "#         return mean, log_std\n",
    " \n",
    "#     def sample(self, state):\n",
    "#         mean, log_std = self.forward(state)\n",
    "#         std = torch.exp(log_std)\n",
    "#         normal = Normal(mean, std)\n",
    "#         z = normal.rsample()\n",
    "#         action = torch.tanh(z) * self.action_bound\n",
    "#         log_prob = normal.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "#         log_prob = log_prob.sum(-1, keepdim=True)\n",
    "#         return action, log_prob\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_bound):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.mean = nn.Linear(256, action_dim)\n",
    "        self.log_std = nn.Linear(256, action_dim)\n",
    "        self.action_bound = action_bound\n",
    " \n",
    "    def forward(self, state):\n",
    "        # x = torch.relu(self.layer1(state))\n",
    "        # x = torch.relu(self.layer2(x))\n",
    "        # x = self.bn1(torch.relu(self.layer1(state)))\n",
    "        # x = self.bn2(torch.relu(self.layer2(x)))\n",
    "        x = torch.nn.functional.leaky_relu(self.layer1(state), 0.01)\n",
    "        x = torch.nn.functional.leaky_relu(self.layer2(x), 0.01)\n",
    "        mean = self.mean(x)\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = torch.clamp(log_std, min=-4, max=1)\n",
    "        std = torch.exp(log_std) + 1e-6  # Adding a small epsilon to avoid std = 0\n",
    "        # print(\"Mean: \",mean)\n",
    "        # print(\"State:\", state)\n",
    "        # print(\"std:  \", std)\n",
    "        return mean, std\n",
    " \n",
    "    def sample(self, state):\n",
    "        # print(\"State:\", state)\n",
    "        mean, std = self.forward(state)\n",
    "        normal = Normal(mean, std)\n",
    "        z = normal.rsample()\n",
    "        action = torch.tanh(z) * self.action_bound\n",
    "        log_prob = normal.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)  # Enforcing action bounds\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.value = nn.Linear(256, 1)\n",
    " \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        value = self.value(x)\n",
    "        return value\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    " \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    " \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(state), np.array(action), np.array(reward, dtype=np.float32), np.array(next_state), np.array(done, dtype=np.float32)\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAC Agent\n",
    "class SACAgent:\n",
    "    def __init__(self, state_dim, action_dim, action_bound):\n",
    "        self.actor = Actor(state_dim, action_dim, action_bound).to(device)\n",
    "        self.critic_1 = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_2 = Critic(state_dim, action_dim).to(device)\n",
    "        self.target_critic_1 = Critic(state_dim, action_dim).to(device)\n",
    "        self.target_critic_2 = Critic(state_dim, action_dim).to(device)\n",
    "        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic_optimizer = optim.Adam(list(self.critic_1.parameters()) + list(self.critic_2.parameters()), lr=3e-4)\n",
    "        self.replay_buffer = ReplayBuffer(1000000)\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.alpha = 0.2\n",
    " \n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        action, _ = self.actor.sample(state)\n",
    "        return action.cpu().data.numpy()\n",
    " \n",
    "    def update(self, batch_size):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        action = torch.FloatTensor(action).to(device)\n",
    "        reward = torch.FloatTensor(reward).to(device).unsqueeze(1)\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        done = torch.FloatTensor(done).to(device).unsqueeze(1)\n",
    " \n",
    "        with torch.no_grad():\n",
    "            next_action, next_log_prob = self.actor.sample(next_state)\n",
    "            target_Q1 = self.target_critic_1(next_state, next_action)\n",
    "            target_Q2 = self.target_critic_2(next_state, next_action)\n",
    "            target_Q = reward + (1 - done) * self.gamma * (torch.min(target_Q1, target_Q2) - self.alpha * next_log_prob)\n",
    " \n",
    "        current_Q1 = self.critic_1(state, action)\n",
    "        current_Q2 = self.critic_2(state, action)\n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    " \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    " \n",
    "        action_new, log_prob_new = self.actor.sample(state)\n",
    "        Q1_new = self.critic_1(state, action_new)\n",
    "        Q2_new = self.critic_2(state, action_new)\n",
    "        actor_loss = (self.alpha * log_prob_new - torch.min(Q1_new, Q2_new)).mean()\n",
    "        # Gradient clipping    \n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1)\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_1.parameters(), 1)\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_2.parameters(), 1)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    " \n",
    "        # Soft update the target network\n",
    "        for target_param, param in zip(self.target_critic_1.parameters(), self.critic_1.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        for target_param, param in zip(self.target_critic_2.parameters(), self.critic_2.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Reward: -706561.5831671506\n",
      "Episode: 1, Reward: -661784.7763244238\n",
      "Episode: 2, Reward: -771798.0514773278\n",
      "Episode: 3, Reward: -689760.0092211156\n",
      "Episode: 4, Reward: -783810.0809612436\n",
      "Episode: 5, Reward: -696290.7614280793\n",
      "Episode: 6, Reward: -754590.7176755683\n",
      "Episode: 7, Reward: -750984.1425293714\n",
      "Episode: 8, Reward: -792090.2312769819\n",
      "Episode: 9, Reward: -669703.3240580729\n",
      "Episode: 10, Reward: -716995.9191947443\n",
      "Episode: 11, Reward: -683971.056110923\n",
      "Episode: 12, Reward: -851412.9264123343\n",
      "Episode: 13, Reward: -618961.2701145584\n",
      "Episode: 14, Reward: -716244.2421027536\n",
      "Episode: 15, Reward: -726142.0686088497\n",
      "Episode: 16, Reward: -711013.6603602632\n",
      "Episode: 17, Reward: -822775.7896523795\n",
      "Episode: 18, Reward: -706572.4552782599\n",
      "Episode: 19, Reward: -789808.6768011305\n",
      "Episode: 20, Reward: -743524.2279347946\n",
      "Episode: 21, Reward: -669322.4387733996\n",
      "Episode: 22, Reward: -702775.372946844\n",
      "Episode: 23, Reward: -717872.7816126568\n",
      "Episode: 24, Reward: -696189.6912042585\n",
      "Episode: 25, Reward: -612371.7079280317\n",
      "Episode: 26, Reward: -843404.5861301911\n",
      "Episode: 27, Reward: -746597.0624538141\n",
      "Episode: 28, Reward: -721872.8327117988\n",
      "Episode: 29, Reward: -801292.056248768\n",
      "Episode: 30, Reward: -705433.0037021199\n",
      "Episode: 31, Reward: -657157.1470256647\n",
      "Episode: 32, Reward: -656535.033351225\n",
      "Episode: 33, Reward: -736044.8490875547\n",
      "Episode: 34, Reward: -695394.9439495732\n",
      "Episode: 35, Reward: -633987.2357984632\n",
      "Episode: 36, Reward: -779922.6594855216\n",
      "Episode: 37, Reward: -751234.2363275818\n",
      "Episode: 38, Reward: -875805.1261351454\n",
      "Episode: 39, Reward: -759631.5691493086\n",
      "Episode: 40, Reward: -706512.687156702\n",
      "Episode: 41, Reward: -765930.4665404989\n",
      "Episode: 42, Reward: -793349.5300288857\n",
      "Episode: 43, Reward: -767018.3041820915\n",
      "Episode: 44, Reward: -723642.1326769891\n",
      "Episode: 45, Reward: -772338.9245063172\n",
      "Episode: 46, Reward: -731149.8525450989\n",
      "Episode: 47, Reward: -700707.6802794386\n",
      "Episode: 48, Reward: -571625.2740191357\n",
      "Episode: 49, Reward: -574073.7445992653\n",
      "Episode: 50, Reward: -857346.4718160917\n",
      "Episode: 51, Reward: -721957.7218228793\n",
      "Episode: 52, Reward: -664172.5370683217\n",
      "Episode: 53, Reward: -664501.8336202636\n",
      "Episode: 54, Reward: -797254.0530672234\n",
      "Episode: 55, Reward: -711751.7130265099\n",
      "Episode: 56, Reward: -769922.4481950885\n",
      "Episode: 57, Reward: -789190.6155863139\n",
      "Episode: 58, Reward: -713101.4457342793\n",
      "Episode: 59, Reward: -667797.6679052375\n",
      "Episode: 60, Reward: -740898.3457197294\n",
      "Episode: 61, Reward: -820052.4041283198\n",
      "Episode: 62, Reward: -790593.0573288623\n",
      "Episode: 63, Reward: -855876.4319143711\n",
      "Episode: 64, Reward: -843282.2430958282\n",
      "Episode: 65, Reward: -705855.3465432411\n",
      "Episode: 66, Reward: -690150.1001705138\n",
      "Episode: 67, Reward: -667398.9084894827\n",
      "Episode: 68, Reward: -774031.4951062582\n",
      "Episode: 69, Reward: -761012.7703666468\n",
      "Episode: 70, Reward: -705910.7454568478\n",
      "Episode: 71, Reward: -657612.8720164327\n",
      "Episode: 72, Reward: -688329.1671549217\n",
      "Episode: 73, Reward: -662634.9913714007\n",
      "Episode: 74, Reward: -613341.1103643\n",
      "Episode: 75, Reward: -720989.6214677973\n",
      "Episode: 76, Reward: -704715.0063600643\n",
      "Episode: 77, Reward: -732536.6294931222\n",
      "Episode: 78, Reward: -751856.3503121913\n",
      "Episode: 79, Reward: -634368.0361200422\n",
      "Episode: 80, Reward: -563853.7276851275\n",
      "Episode: 81, Reward: -789336.5023382568\n",
      "Episode: 82, Reward: -767183.4089743085\n",
      "Episode: 83, Reward: -703331.2857996458\n",
      "Episode: 84, Reward: -738473.9138178317\n",
      "Episode: 85, Reward: -695534.6063308072\n",
      "Episode: 86, Reward: -712242.0912754831\n",
      "Episode: 87, Reward: -814310.6486032538\n",
      "Episode: 88, Reward: -756662.3741996845\n",
      "Episode: 89, Reward: -701853.109642142\n",
      "Episode: 90, Reward: -846552.782839198\n",
      "Episode: 91, Reward: -769671.4769203962\n",
      "Episode: 92, Reward: -693388.9125279954\n",
      "Episode: 93, Reward: -672205.8043530887\n",
      "Episode: 94, Reward: -709964.311856251\n",
      "Episode: 95, Reward: -737791.4772685029\n",
      "Episode: 96, Reward: -762407.7219002651\n",
      "Episode: 97, Reward: -812140.9922744543\n",
      "Episode: 98, Reward: -785449.7733994306\n",
      "Episode: 99, Reward: -720006.1813895572\n"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "def train():\n",
    "    env = QuadrotorEnv()\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    action_bound = env.action_space.high[0]\n",
    "    agent = SACAgent(state_dim, action_dim, action_bound)\n",
    "    episodes = 100\n",
    "    steps_per_episode = 1000\n",
    "    batch_size = 256\n",
    "    gradient_steps = 50\n",
    " \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        for _ in range(steps_per_episode):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # for _ in range(gradient_steps):\n",
    "        #     agent.update(batch_size)\n",
    " \n",
    "        print(f'Episode: {episode}, Reward: {episode_reward}')\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
