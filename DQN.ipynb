{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# import gym\n",
    "# from gym import spaces\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ReplayBuffer import ReplayBuffer\n",
    "from environment import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(state = [0, 1, 0, -1, 0, 0], mu = 0.05, m = 1, g = 9.81, thetamin = 0, thetamax = np.pi, phimin = 0, phimax = 2*np.pi, Tmin = 0, Tmax = 20, dt = 0.02, dphi = 0.0175, dtheta = 0.0175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, lr, input_dim, fc1_dims, fc2_dims, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, fc1_dims)\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
    "\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
    "\n",
    "        f3 = 0.003\n",
    "        self.fc3 = nn.Linear(fc2_dims, output_dim)\n",
    "        T.nn.init.uniform_(self.fc3.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.fc3.bias.data, -f3, f3)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        self.to(device)\n",
    "    def forward(self, x):\n",
    "        x = T.relu(self.bn1(self.fc1(x)))\n",
    "        x = T.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent(object):\n",
    "    def __init__(self, lr, input_dim, input_dims_buff, fc1_dims, fc2_dims, gamma, n_actions, epsilon, batch_size,\n",
    "                 buffer_size=1000000, eps_end=0.01, eps_dec=5e-7, N = 100):\n",
    "        self.lr = lr\n",
    "        self.input_dim = input_dim\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.memory = ReplayBuffer(buffer_size, input_dims_buff)\n",
    "        self.QNetwork = DQN(lr, input_dim, fc1_dims, fc2_dims, n_actions)\n",
    "        self.QNetwork_target = DQN(lr, input_dim, fc1_dims, fc2_dims, n_actions)\n",
    "        self.update_network_parameters(tau = 1)\n",
    "    \n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        qn_params = self.QNetwork.named_parameters()\n",
    "        target_qn_params = self.QNetwork_target.named_parameters()\n",
    "        qn_dict = dict(qn_params)\n",
    "        target_qn_dict = dict(target_qn_params)\n",
    "        for name in qn_dict:\n",
    "            qn_dict[name] = tau*qn_dict[name].clone() + (1-tau)*target_qn_dict[name].clone()\n",
    "        self.QNetwork_target.load_state_dict(qn_dict)\n",
    "\n",
    "    def choose_action(self, state, epsilon = None):\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "        self.QNetwork.eval()\n",
    "        if np.random.random() > epsilon:\n",
    "            state = T.tensor([state], dtype=T.float).to(device)\n",
    "            actions = self.QNetwork.forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.n_actions)\n",
    "        self.QNetwork.train()\n",
    "        return env.actionspace()[action]\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.add_sample(state, action, reward, new_state, done)\n",
    "    \n",
    "    def learn(self, step):\n",
    "        if self.memory.count < self.batch_size:\n",
    "            return\n",
    "        states, actions, rewards, new_states, dones = self.memory.return_sample(self.batch_size)\n",
    "\n",
    "        states = T.tensor(states, dtype=np.float32).to(device)\n",
    "        actions = T.tensor(actions, dtype=np.float32).to(device)\n",
    "        rewards = T.tensor(rewards, dtype=np.float32).to(device)\n",
    "        new_states = T.tensor(new_states, dtype=np.float32).to(device)\n",
    "        dones = T.tensor(dones).to(device)\n",
    "\n",
    "        self.QNetwork.eval()\n",
    "        all_qsas = self.QNetwork.forward(states)\n",
    "        opt_qsas = T.max(all_qsas, dim = 1)\n",
    "        self.QNetwork_target.eval()\n",
    "        next_qsas = self.QNetwork_target.forward(new_states)\n",
    "        next_opt_qsas = T.max(next_qsas, dim = 1)\n",
    "\n",
    "        bellmanoptimality = []\n",
    "        for j in range(self.batch_size):\n",
    "            bellmanoptimality.append(rewards[j] + self.gamma*next_opt_qsas[j]*dones[j])\n",
    "        bellmanoptimality = T.tensor(bellmanoptimality).to(device)\n",
    "        bellmanoptimality = bellmanoptimality.view(self.batch_size, 1)\n",
    "\n",
    "        self.QNetwork.train()\n",
    "        self.QNetwork.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(bellmanoptimality, opt_qsas)\n",
    "        loss.backward()\n",
    "        self.QNetwork.optimizer.step()\n",
    "\n",
    "        if step % self.N == 0:\n",
    "            self.update_network_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "Agent = DQN_Agent(lr=0.0001, input_dim=6, input_dims_buff = [6,3], fc1_dims=256, fc2_dims=256, gamma=0.99, n_actions=3, epsilon=0.3, batch_size=64, buffer_size=1000000, eps_end=0.2, eps_dec=0.0001, N=100)\n",
    "\n",
    "num_episodes = 50\n",
    "max_steps = 300\n",
    "\n",
    "Average_Rewards = []\n",
    "for i in tqdm(range(num_episodes)):\n",
    "    done = False\n",
    "    step = 0\n",
    "    state = [0, 1, 0, -1, 0, 0]\n",
    "    total_reward = 0\n",
    "    while not done and step < max_steps:\n",
    "        action = Agent.choose_action(state)\n",
    "        print(step)\n",
    "        new_state, reward, done = env.infostep(step, action)\n",
    "        Agent.remember(state, action, reward, new_state, done)\n",
    "        Agent.learn(step)\n",
    "        print(step)  \n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "    Average_Rewards.append(total_reward)\n",
    "    if epsilon > Agent.eps_min:\n",
    "        epsilon = epsilon - Agent.eps_dec\n",
    "    print('episode ', i, 'score %.1f' % total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Average_Rewards)), Average_Rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Average Reward vs Episode')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
