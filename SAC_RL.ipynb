{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QuadrotorEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['console']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(QuadrotorEnv, self).__init__()\n",
    "\n",
    "        # Constants\n",
    "        self.g = 9.81  # gravity\n",
    "        self.m = 1.0   # mass of the UAV\n",
    "        self.mu = 0.05 # damping factor\n",
    "        self.dt = 0.02 # time step\n",
    "\n",
    "        # Define action and observation space\n",
    "        # Actions are thrust T, angle phi, angle theta\n",
    "        self.action_space = spaces.Box(low=np.array([0, -np.pi, -np.pi]), \n",
    "                                       high=np.array([20, np.pi, np.pi]), dtype=np.float32)\n",
    "\n",
    "        # Observation space: x, y, z, vx, vy, vz\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)\n",
    "\n",
    "        # State initialization\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        T, phi, theta = action\n",
    "        x, y, z, vx, vy, vz = self.state\n",
    "\n",
    "        # Calculate accelerations\n",
    "        ax = (-0.7071 * np.cos(phi) * np.sin(theta) - 0.7071 * np.sin(phi)) * T / self.m\n",
    "        ay = (-0.7071 * np.cos(phi) * np.sin(theta) - 0.7071 * np.sin(phi)) * T / self.m\n",
    "        az = self.g - (np.cos(phi) * np.cos(theta)) * T / self.m\n",
    "\n",
    "        # Update velocities\n",
    "        vx += (ax - self.mu * vx) * self.dt\n",
    "        vy += (ay - self.mu * vy) * self.dt\n",
    "        vz += (az - self.mu * vz) * self.dt\n",
    "\n",
    "        # Update positions\n",
    "        x += vx * self.dt\n",
    "        y += vy * self.dt\n",
    "        z += vz * self.dt\n",
    "\n",
    "        # Update state\n",
    "        self.state = np.array([x, y, z, vx, vy, vz])\n",
    "\n",
    "        # Calculate reward (placeholder)\n",
    "        reward = -np.sqrt((x - 5 * np.cos(1.2 * self.current_step * self.dt))**2 + (y - 5 * np.sin(1.2 * self.current_step * self.dt))**2 + (z + 20)**2)\n",
    "\n",
    "        # Check if UAV is within the reasonable bounds (this is a simple check)\n",
    "        done = z < -25 or self.current_step > 1000\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Optionally we could add more info\n",
    "        info = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state\n",
    "        self.state = np.array([0.0, 0.0, 0.0, 1.0, -1.0, 0.0], dtype=np.float32)\n",
    "        self.current_step = 0\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        if mode == 'console':\n",
    "            print(f'State: {self.state}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SAC Agent\n",
    "class SAC_Agent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, lr=0.0003):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lr = lr\n",
    "\n",
    "        # Actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Q networks\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Target Q networks\n",
    "        self.target_q1 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.target_q2 = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        # Initialize target Q networks with Q networks\n",
    "        self.target_q1.load_state_dict(self.q1.state_dict())\n",
    "        self.target_q2.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.q_optimizer = optim.Adam(list(self.q1.parameters()) + list(self.q2.parameters()), lr=lr)\n",
    "\n",
    "        # Initialize policy network\n",
    "        # self.policy = PolicyNetwork(state_dim, action_dim, hidden_dim)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action_mean = self.actor(state)\n",
    "        dist = torch.distributions.Normal(action_mean, torch.ones_like(action_mean) * 0.1)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.detach().numpy()[0], log_prob.detach()\n",
    "\n",
    "    # def choose_action(self, state):\n",
    "    #     state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    #     action_mean, action_log_std = self.policy(state)\n",
    "    #     std = torch.exp(action_log_std)\n",
    "    #     normal = torch.distributions.Normal(action_mean, std)\n",
    "    #     sampled_action = normal.sample()\n",
    "    #     return sampled_action.detach().numpy()[0]  # Return thrust, roll, and pitch values separately\n",
    "\n",
    "\n",
    "    def train_agent(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch, gamma=0.99, alpha=0.2, tau=0.005):\n",
    "        state_batch = torch.FloatTensor(state_batch)\n",
    "        action_batch = torch.FloatTensor(action_batch)\n",
    "        reward_batch = torch.FloatTensor(reward_batch)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch)\n",
    "        done_batch = torch.FloatTensor(done_batch)\n",
    "\n",
    "        # Update Q networks\n",
    "        next_actions, next_log_probs = self.choose_action(next_state_batch)\n",
    "        target_q1_next = self.target_q1(torch.cat([next_state_batch, next_actions], dim=-1))\n",
    "        target_q2_next = self.target_q2(torch.cat([next_state_batch, next_actions], dim=-1))\n",
    "        target_min_q_next = torch.min(target_q1_next, target_q2_next) - alpha * next_log_probs\n",
    "        target_q = reward_batch + gamma * (1 - done_batch) * target_min_q_next.detach()\n",
    "\n",
    "        q1_loss = nn.MSELoss()(self.q1(torch.cat([state_batch, action_batch], dim=-1)), target_q)\n",
    "        q2_loss = nn.MSELoss()(self.q2(torch.cat([state_batch, action_batch], dim=-1)), target_q)\n",
    "\n",
    "        self.q_optimizer.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        q2_loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "\n",
    "        # Update actor network and alpha\n",
    "        actions, log_probs = self.choose_action(state_batch)\n",
    "        q1 = self.q1(torch.cat([state_batch, actions.unsqueeze(-1)], dim=-1))\n",
    "        q2 = self.q2(torch.cat([state_batch, actions.unsqueeze(-1)], dim=-1))\n",
    "        min_q = torch.min(q1, q2)\n",
    "\n",
    "        actor_loss = (alpha * log_probs - min_q).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target Q networks\n",
    "        for target_param, param in zip(self.target_q1.parameters(), self.q1.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "        for target_param, param in zip(self.target_q2.parameters(), self.q2.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Main training loop\n",
    "# env = QuadrotorEnv() \n",
    "# state_dim = env.observation_space.shape[0]\n",
    "# action_dim = env.action_space.shape[0]\n",
    "\n",
    "# agent = SAC_Agent(state_dim, action_dim)\n",
    "\n",
    "# EPISODES = 1000\n",
    "# for episode in range(EPISODES):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     total_reward = 0\n",
    "#     while not done:\n",
    "#         action, _ = agent.choose_action(state)\n",
    "#         next_state, reward, done, _ = env.step(action)\n",
    "#         agent.train_agent(state, action, reward, next_state, done)\n",
    "#         state = next_state\n",
    "#         total_reward += reward\n",
    "#     print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the environment\n",
    "# env = QuadrotorEnv() \n",
    "\n",
    "# # Initialize SAC agent\n",
    "# agent = SAC_Agent(state_dim, action_dim)\n",
    "\n",
    "# # Initialize replay buffer\n",
    "# replay_buffer = deque(maxlen=10000)\n",
    "\n",
    "# num_iterations = 10\n",
    "# environment_steps = 10\n",
    "# gradient_steps = 10\n",
    "\n",
    "\n",
    "# # Main training loop\n",
    "# for iteration in range(num_iterations):\n",
    "#     # Environment steps\n",
    "#     for step in range(environment_steps):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         while not done:\n",
    "#             thrust, phi, theta = agent.choose_action(state)\n",
    "#             action = np.array([thrust, phi, theta])\n",
    "#             next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "#             # action = agent.choose_action(state)\n",
    "#             # next_state, reward, done, _ = env.step(action)\n",
    "#             replay_buffer.append((state, action, reward, next_state))\n",
    "#             state = next_state\n",
    "\n",
    "#     # Gradient steps\n",
    "#     for gradient_step in range(gradient_steps):\n",
    "#         # Sample batch from replay buffer\n",
    "#         batch_size = min(batch_size, len(replay_buffer))\n",
    "#         batch = random.sample(replay_buffer, batch_size)\n",
    "#         states, actions, rewards, next_states = zip(*batch)\n",
    "\n",
    "#         # Update Q-function parameters\n",
    "#         agent.update_q(states, actions, rewards, next_states)\n",
    "\n",
    "#         # Update policy weights\n",
    "#         agent.update_policy(states)\n",
    "\n",
    "#         # Adjust temperature\n",
    "#         agent.update_alpha(states)\n",
    "\n",
    "#         # Update target network weights\n",
    "#         agent.update_target_networks()\n",
    "\n",
    "# # Output: θ1, θ2, φ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main training loop\n",
    "# env = QuadrotorEnv() \n",
    "# state_dim = env.observation_space.shape[0]\n",
    "# action_dim = env.action_space.shape[0]\n",
    "\n",
    "# agent = SAC_Agent(state_dim, action_dim)\n",
    "\n",
    "# replay_buffer = deque(maxlen=100000)  # Replay buffer\n",
    "\n",
    "# NUM_ITERATIONS = 10\n",
    "# ENVIRONMENT_STEPS = 10\n",
    "# GRADIENT_STEPS = 10\n",
    "\n",
    "# for iteration in range(NUM_ITERATIONS):\n",
    "#     # Environment step\n",
    "#     for step in range(ENVIRONMENT_STEPS):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         while not done:\n",
    "#             action = agent.choose_action(state)\n",
    "#             next_state, reward, done, _ = env.step(action)\n",
    "#             replay_buffer.append((state, action, reward, next_state))\n",
    "#             state = next_state\n",
    "    \n",
    "#     # Gradient step\n",
    "#     for _ in range(GRADIENT_STEPS):\n",
    "#         # Sample a mini-batch from replay buffer\n",
    "#         batch_indices = np.random.choice(len(replay_buffer), BATCH_SIZE, replace=False)\n",
    "#         batch = [replay_buffer[i] for i in batch_indices]\n",
    "#         state_batch, action_batch, reward_batch, next_state_batch = zip(*batch)\n",
    "        \n",
    "#         # Update Q-function parameters\n",
    "#         agent.update_q(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "        \n",
    "#         # Update policy weights\n",
    "#         agent.update_policy(state_batch)\n",
    "        \n",
    "#         # Adjust temperature\n",
    "#         agent.adjust_temperature()\n",
    "        \n",
    "#         # Update target network weights\n",
    "#         agent.update_target_networks()\n",
    "\n",
    "# # Output: θ1, θ2, φ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the environment\n",
    "# env = QuadrotorEnv() \n",
    "\n",
    "# # Initialize replay buffer D\n",
    "# replay_buffer = deque(maxlen=10000)\n",
    "\n",
    "# # Initial parameters\n",
    "# theta1 = agent.q1.state_dict()\n",
    "# theta2 = agent.q2.state_dict()\n",
    "# phi = agent.actor.state_dict()\n",
    "\n",
    "# # Initialize target network weights\n",
    "# target_theta1 = theta1.copy()\n",
    "# target_theta2 = theta2.copy()\n",
    "\n",
    "# # Hyperparameters\n",
    "# lambda_q = 0.001\n",
    "# lambda_pi = 0.001\n",
    "# lambda_alpha = 0.001\n",
    "# tau = 0.005\n",
    "\n",
    "\n",
    "# NUM_ITERATIONS = 10\n",
    "# NUM_ENVIRONMENT_STEPS = 10\n",
    "# NUM_GRADIENT_STEPS = 10\n",
    "\n",
    "# # Main training loop\n",
    "# for iteration in range(NUM_ITERATIONS):\n",
    "#     # Environment step\n",
    "#     for step in range(NUM_ENVIRONMENT_STEPS):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         while not done:\n",
    "#             # Sample action from the policy\n",
    "#             action = agent.choose_action(state)\n",
    "#             next_state, reward, done, _ = env.step(action)\n",
    "#             # Store the transition in the replay pool\n",
    "#             replay_buffer.append((state, action, reward, next_state))\n",
    "#             state = next_state\n",
    "\n",
    "#     # Gradient step\n",
    "#     for gradient_step in range(NUM_GRADIENT_STEPS):\n",
    "#         # Update Q-function parameters\n",
    "#         q1_loss = update_q_function(replay_buffer, agent.q1, agent.target_q1, agent.q_optimizer, theta1, theta2, phi, lambda_q, tau)\n",
    "#         q2_loss = update_q_function(replay_buffer, agent.q2, agent.target_q2, agent.q_optimizer, theta2, theta1, phi, lambda_q, tau)\n",
    "\n",
    "#         # Update policy weights\n",
    "#         policy_loss = update_policy(replay_buffer, agent.policy, agent.policy_optimizer, theta1, theta2, phi, lambda_pi)\n",
    "\n",
    "#         # Adjust temperature\n",
    "#         alpha_loss = update_temperature(replay_buffer, agent.log_alpha, agent.alpha_optimizer, lambda_alpha)\n",
    "\n",
    "#         # Update target network weights\n",
    "#         update_target_network(target_theta1, theta1, tau)\n",
    "#         update_target_network(target_theta2, theta2, tau)\n",
    "\n",
    "#     # Output parameters\n",
    "#     theta1 = target_theta1.copy()\n",
    "#     theta2 = target_theta2.copy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
